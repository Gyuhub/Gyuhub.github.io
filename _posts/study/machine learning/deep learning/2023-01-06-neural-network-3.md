---
layout: post
title: Neural-Network-3
category: deep learning
post-order: 5
---

# 3층 신경망 구현 {#3-layer-neural-network}

지금까지 배운 여러 이론과 코드를 통해서 본격적으로 신경망을 구현해보겠습니다. 이를 위해서 **Python** 언어를 사용하고 **numpy**와 **matplotlib** 모듈만 사용할 예정입니다.

<figure>
    <img src="/assets/images/study/machine_learning/deep_learning/2023-01-06-neural_network_6.jpg"
          title="3-layer neural network"
          alt="Images of 3-layer neural network"
          class="img_center"
          style="width: 50%"/>
     <figcaption>3층 신경망</figcaption>
</figure>

**3층 신경망**은 위의 그림처럼 입력층은 2개, 첫번째 은닉층은 3개, 두번째 은닉층은 2개 그리고 출력층은 2개의 뉴런으로 구성할 예정입니다.
신경망에서 층이 깊어질수록 **가중치**와 **편향 매개변수**$(w,\,b)$의 개수도 많아질 것입니다. 활성화 함수의 **입력**을 계산하기 위해서 입력층의 뉴런들과 매개변수들을 일일이 곱한다면 <ins>연산량이 매우 많아지겠죠</ins>.

> :buld: 따라서 이러한 큰 비용의 연산을 효율적으로 하기 위해서 **행렬**을 사용하고 행렬 계산에 특화되어 있는 Python 모듈이 바로 **numpy**입니다.

# 신경망 행렬

신경망 내부 연산에서 사용되는 행렬은 바로 신경망 내부층 각각의 뉴런의 정보를 담은 행렬과 매개변수들의 행렬입니다. 하나씩 차례대로 살펴보겠습니다.

## 입력층

먼저 입력층의 행렬에는 말 그대로 **입력 값**들이 들어있습니다. 예를 들어 입력이 $x_1,\,x_2$의 두 개의 입력을 받는 네트워크라면 $x=\begin{bmatrix} x_1 & x_2 \end{bmatrix}^T$처럼 **열 벡터**로 그 값들을 순서대로 나열하여 표현할 수 있습니다.

> :note: **편향**의 경우에는 입력 값이 항상 **1**로 고정되어 있기에 그림에도 생략하고 입력 값의 행렬에도 포함하지 않습니다.

## 은닉층

다음으로 은닉층입니다. 사실 은닉층은 그 이름대로 **숨겨져있기에**(hidden) 내부에 어떤 값들이 들어있는지 외부에서는 알 수 없습니다. 따라서 행렬 계산 결과를 *임시적*으로 저장할 뿐 따로 그 값을 저장해서 활용하지는 않습니다.

## 출력층

출력층의 행렬에는 입력층과 같이 **출력 값**들이 들어있습니다. 예를 들어 출력이 $y_1, y_2, y_3$의 세 개의 출력이 나오는 네트워크라면 $y=\begin{bmatrix} y_1 & y_2 & y_3 \end{bmatrix}^T$처럼 **열 벡터**로 그 값들을 순서대로 나열해서 표현할 수 있습니다.

## 매개변수

신경망에서 입력이나 출력만큼 중요한 역할을 하는 매개변수입니다. 매개변수에는 **가중치 매개변수**와 **편향 매개변수**가 있습니다. 

매개변수들은 신경망층 사이의 뉴런과 뉴런 사이에서 이전층의 뉴런에서 받은 입력을 다음층의 뉴런의 활성화 함수로 넘겨주는 과정에서 **연산**에 관여합니다. 따라서 이전층과 다음층의 **뉴런 모두**에게 영향을 미치는 것입니다.

먼저 가중치 매개변수의 행렬에 대해서 살펴보겠습니다. 가중치 매개변수 행렬 $w$는 아래와 같이 수식적으로 나타낼 수 있습니다.

$$
w^{(n)}=\begin{bmatrix}
w^{(n)}_{11} & w^{(n)}_{21} & \cdots & w^{(n)}_{j1} \\
w^{(n)}_{12} & w^{(n)}_{22} & \cdots & w^{(n)}_{j2} \\
\vdots & \vdots & \ddots & \vdots \\
w^{(n)}_{1i} & w^{(n)}_{2i} & \cdots & w^{(n)}_{ji} \\
\end{bmatrix} \label{weight_matrix} \tag{1}
$$

식 $(\ref{weight_matrix})$에서 가중치 $w$위의 $\boldsymbol{(n)}$은 해당 가중치 행렬이 $\boldsymbol{n}$**번째 층**의 가중치들의 행렬임을 나타냅니다. 행렬 내부의 원소들을 통해 이전층$\boldsymbol{(n-1)}$의 입력 값들을 다음층$\boldsymbol{(n)}$의 활성화 함수의 입력을 연산합니다.

가중치 $w$의 **아래 첨자**는 이전층과 다음층의 **뉴런의 순서**를 나타내는 번호입니다. 어떤 층의 뉴런들을 가장 **위쪽부터 차례대로** 번호를 매긴다면 아래 첨자의 **앞**의 번호는 ***다음층***의 뉴런의 번호를, **뒤**의 번호는 ***이전층***의 뉴런의 번호를 나타냅니다. 예를 들어 [Fig. 1.](#3-layer-neural-network)과 같은 신경망이 있을때 2층의 가중치 매개변수 행렬은 아래와 같습니다.