---
layout: post
title: Introduction to Reinforcement Learning-2
category: reinforcement learning
post-series: Reinforcement learning from scratch
post-order: 3
---

이전 [post](https://gyuhub.github.io/posts/study/machine%20learning/reinforcement%20learning/introduction-to-rl-1)에서는 강화 학습의 **역사**와 기계 학습의 **분류**에 대해서 배웠습니다. 이번 post에서는 강화 학습을 이루는 **기본 개념**들에 대해서 알아보겠습니다.

---

# 순차적 의사결정 문제

강화 학습을 통해 해결하고자 하는 문제는 **순차적 의사결정 문제**(sequential decision problem)입니다. 순차적 의사결정 문제는 다소 낯설게 들릴 수 있지만, 일상 속에서도 충분히 맞닥뜨릴 수 있는 문제입니다. 예시를 하나 들어보겠습니다.

<figure>
    <img src="/posts/study/machine learning/reinforcement learning/images/introduction_to_rl_6.png"
         title="Example of sequential decision problem"
         alt="Image of example of sequential decision problem"
         class="img_center"
         style="width: 20%"/>
    <figcaption>순차적 의사결정 문제의 예시</figcaption>
</figure>

우리는 매일 빠짐없이 하는 것이 있습니다. 바로 건강을 유지하고 에너지를 얻기 위해 식사를 합니다. 식사를 할 때 어떤 단계를 거치게 될까요? 직접 요리를 해서 식사를 하는 경우에는 아래와 같은 단계로 구성될 수 있겠습니다.

<i class="fa-solid fa-1"></i>. 식재료를 구매한다.

<i class="fa-solid fa-2"></i>. 요리를 한다.

<i class="fa-solid fa-3"></i>. 식사를 한다.

<i class="fa-solid fa-4"></i>. 뒷정리(설거지)를 한다.

그리고 이 4단계는 반드시 순서에 맞게 이루어져야 합니다. 만일 순서가 뒤바뀌는 경우에는 어떻게 될까요?

* <i class="fa-solid fa-4"></i>. 뒷정리를 하고, <i class="fa-solid fa-2"></i>. 요리를 하고, <i class="fa-solid fa-1"></i>. 식재료를 구매하고, <i class="fa-solid fa-3"></i>. 식사를 한다.
  * 남아있는 식재료로 요리는 어떻게든 하더라도, 뒷정리가 안된 상태에다가 식재료도 또 남겠네요😂.
* <i class="fa-solid fa-3"></i>. 식사를 하고, <i class="fa-solid fa-4"></i>. 뒷정리를 하고, <i class="fa-solid fa-1"></i>. 식재료를 구매하고, <i class="fa-solid fa-2"></i>. 요리를 한다.
  * 식사를 끝내고 또 요리를 하면 누가 먹나요😅?
* <i class="fa-solid fa-1"></i>. 식재료를 구매하고, <i class="fa-solid fa-2"></i>. 요리를 하고, <i class="fa-solid fa-4"></i>. 뒷정리를 하고, <i class="fa-solid fa-3"></i>. 식사를 한다.
  * 요리를 끝내자마자 뒷정리하는 것은 좋은데, 식기전에 식사는 하고 하는게 어떨까요?

이처럼 일상적이고 몹시 익숙한 과정이라 하더라도 이를 성공적으로 잘 마무리 하기 위해서는 우리는 몇 가지 **의사결정**을 **순차적으로** 해 주어야 합니다. 어떤 행동(의사결정)을 하고, 그로 인해서 상황이 바뀌고, 바뀐 상황에서 또다른 행동을 하는 **연속적인 흐름**을 잘 풀어야 하는 문제가 바로 순차적 의사결정 문제입니다. 순차적 의사결정 문제의 또다른 예시로는 운전, 게임 등이 있습니다.

세상에는 정말 많고 다양한 **순차적 의사결정 문제**들이 존재합니다. 복잡한 세상속에서는 단순히 행동 하나로만 그 결과가 성공이나 실패 둘 중 하나로 직결되지 않고 유기적으로 연결된 채 계속해서 행동을 요구하기 때문입니다. 그래서 세상에는 순차적 의사결정 문제가 아닌 것을 찾기가 더 어려울 정도로 많은 상황들을 **순차적 의사결정 문제**로 생각할 수 있습니다. 그리고 강화 학습은 이런 중요한 문제를 풀기 위해 고안된 방법론입니다. 얼마나 강화 학습이 인공 지능에서 중요한 비중을 차지할지 감이 오시나요?

## 도식화

앞서 설명한 순차적 의사결정 문제를 강화 학습에 초점을 맞춰서 도식화하면 아래와 같습니다.

<figure>
    <img src="/posts/study/machine learning/reinforcement learning/images/introduction_to_rl_7.png"
         title="Schematics of sequential decision problem"
         alt="Image of schematics of sequential decision problem"
         class="img_center"
         style="width: 60%"/>
    <figcaption>순차적 의사결정 문제의 도식화</figcaption>
</figure>

**에이전트**(agent)가 **액션**(행동, action)을 하고 그에 따라 상황이 변하는 것을 하나의 **루프**(loop)라고 했을때, 이 루프가 끊임없이 **반복**되는 것을 <ins>순차적 의사결정 문제</ins>라고 할 수 있습니다. [Fig. 2.]에서 에이전트, **환경**(environment), **상태**(state) 그리고 **보상**(reward)이라는 개념이 처음 등장한만큼 그에 대해 먼저 자세히 살펴보겠습니다. 

---

# 에이전트

**에이전트**는 강화 학습의 **주인공**이자 **주체**입니다. 학습하는 **대상**이며 동시에 *환경 속에서* 행동하는 개체를 가리키는 용어입니다. 위에서의 예시에서는 요리하고 식사를 하는 객체가 에이전트가 되겠습니다. 에이전트는 어떤 **액션**을 할지 정하는 것이 가장 주된 역할입니다. 에이전트의 입장에서 위의 루프는 구체적으로 다음 3단계로 이루어져 있습니다.

<i class="fa-solid fa-1"></i>. 현재 상황 $s_t$에서 어떤 액션$(a_t)$을 해야 할지 결정.

<i class="fa-solid fa-2"></i>. 결정된 액션 $a_t$를 환경으로 보냄.

<i class="fa-solid fa-3"></i>. 환경으로부터 그에 따른 보상과 다음 상태의 정보를 받음

# 환경

**환경**은 에이전트를 제외한 모든 요소를 말합니다. 그래서 어떤 요소가 환경이라고 딱 집어서 말하기가 어렵습니다. 이전의 예시에서는 식사를 하는 객체(에이전트)를 **제외한 모든 것**이 환경입니다. 다른 말로는 에이전트가 어떤 **액션**을 했을 때, 그 결과에 영향을 아주 조금이라도 미치는 모든 요소들이 환경이라고 할 수 있습니다. 환경 속에서 에이전트가 어떤 액션을 취하고 나면 에이전트의 **상태**가 바뀔 수 있습니다.

# 상태

**상태**는 현재 에이전트에 대한 모든 **정보**를 **숫자**로 기록해서 표현한 것을 말합니다. 현재 가지고 있는 돈, 요리 완료까지 남은 시간, 접시의 개수 등등을 숫자로 엮은 하나의 벡터처럼 생각할수도 있습니다. 환경은 이 **상태 변화**(state transition)를 일으키는 역할을 담당합니다. 액션에 대한 결과를 알려주는 것입니다. 이를 종합하면 환경이 하는 일은 다음과 같은 단계로 이루어집니다.

<i class="fa-solid fa-1"></i>. 상태 $s_t$에서 에이전트로부터 받은 액션 $a_t$를 통해서 상태 변화를 일으킴.

<i class="fa-solid fa-2"></i>. 그 결과 상태는 $s_t \rightarraw s_{t+1}$로 바뀜.

<i class="fa-solid fa-3"></i>. 에이전트에게 줄 보상 $r_t$도 함께 계산.

<i class="fa-solid fa-4"></i>. $s_{t+1}$과 $r_{t+1}$을 에이전트에게 전달.

위와 같은 단계를 1부터 4까지 한번 반복하면, 즉 **에이전트**와 **환경**이 한 번 상호 작용하면 하나의 **루프**가 끝납니다. 이를 한 **틱**(tick)이 지났다고도 표현합니다. 실제 세계는 앞의 그림과 다르게 시간의 흐름이 **연속적**(continuous)이겠지만 순차적 의사결정 문제에서는 시간의 흐름을 **이산적**(discrete)으로 생각합니다. 그리고 그 시간의 단위를 틱 혹은 **타임 스텝**(time step)이라고 합니다.

# 보상

**보상**이란 의사결정을 얼마나 잘하고 있는지 알려주는 신호입니다. 그리고 강화 학습의 목적은 과정에서 받는 보상의 총합, 즉 **누적 보상**(cumulative reward)을 최대화하는 것입니다. 에이전트는 **좋은** 액션을 하면 보상을 크게 받고, **나쁜** 액션을 하면 보상을 적게 받습니다. 따라서 보상을 통해 에이전트는 액션을 **교정**할 **힌트**를 얻게 됩니다. 보상은 강화 학습을 이해하는 데 있어서 가장 중요하고 필수적인 개념입니다. 만약 보상이 존재하지 않는다면, 에이전트는 아무것도 배울 수 없습니다.

보상에는 3가지 특징이 있습니다.

<i class="fa-solid fa-1"></i>. 보상은 **어떻게**가 아닌 **얼마나**를 나타낸다.

첫 번째 특징은 보상은 "**어떻게**"에 대한 정보를 담고 있지 않다는 점입니다. 보상은 **에이전트**가 특정한 액션을 하면 그것에 대해 "**얼마나**" 잘 하고 있는지 평가를 해주는 지표일뿐, **어떻게** 해야 높은 보상을 얻을 수 있을지 알려주지 않습니다. 이러한 점이 지도 학습과 강화 학습을 명확하게 구분할 수 있게 해줍니다. 그렇다면 어떻게해야 높은 보상을 얻을 수 있는지 알려주지 않음에도 학습을 잘할 수 있을까요? 그것은 바로 수많은 **시행착오**(trial & error)덕분입니다.

에이전트는 자신이 처한 상황속에서 특정한 액션을 취함으로써 그 액션에 대한 평가로 **보상**이라는 신호를 받습니다. 에이전트가 자신이 했던 액션을 **교정**할 수 있는 이유는 에이전트가 좋은 액션을 하면 높은 보상을, 나쁜 액션을 하면 낮은 보상일 주면서 액션의 좋고 나쁨을 평가해 주는 신호가 있기 때문입니다. 보상이 **직접적으로** <ins>어떻게 해야 할지</ins>를 알려주지는 않지만, <ins>보상이 높았던 액션을 많이 하고 보상이 낮았던 액션을 적게 함으로써</ins> 액션을 조금씩 교정할 수 있게 해주는 것입니다.

<i class="fa-solid fa-2"></i>. 보상은 **스칼라**값이다.

두 번째 특징은 보상은 **벡터**가 아닌 **스칼라**라는 점입니다. 만약 보상이 벡터라면 동시에 2개 이상의 값을 최대화하는 것을 목표로 할 수 있겠지만, 보상은 스칼라이기 때문에 오직 하나의 값만 최대화할 수 있습니다. 왜 벡터가 아닌 스칼라값으로 보상을 설정했을까요? 예시를 하나 들어보겠습니다.

<figure>
    <img src="/posts/study/machine learning/reinforcement learning/images/introduction_to_rl_8.png"
         title="Example of vector reward problem"
         alt="Image of example of vector reward problem"
         class="img_center"
         style="width: 40%"/>
    <figcaption>벡터 보상의 문제점의 예시</figcaption>
</figure>

[Fig. 3.]과 같이 게임속에서 자율주행을 하는 강화 학습 에이전트가 있다고 가정해보겠습니다. 해당 에이전트는 현재 자율주행을 하면서 횡단보도가 있는 도로를 지나고 있습니다. 이때 에이전트는 최대한 오랫동안 주행하기 위해서 멈추지 않고 1초 주행을 할때마다 **+5**의 보상을 받는다고 하겠습니다. 또한, 일반국도에는 횡단보도가 존재하고 보행신호가 켜지면 사람이 지나갑니다. 그래서 사람의 안전을 위해서 사람과 충돌하지 않고 주행하면 **+1**의 보상을 주겠습니다. 이를 벡터로 나타낸다면 아래의 수식처럼 표현할 수 있습니다.

$$
r_t = \begin{bmatrix}
  5t \\
  h_t
\end{bmatrix}, \\
h_t = \begin{cases}
  1\ (\text{when the car does not hit a person}) \\
  0\ (\text{otherwise})
\end{cases}. \label{vector_reward} \tag{1}
$$

식 $(\ref{vector_reward})$에서 $t$는 시점 t를 나타내고 $h_t$는 사람과 에이전트가 부딪히지 않았을때만 1이라는 결과를 출력합니다. 언뜻 보기에는 큰 문제가 없어보이지만 만약 아래와 같은 그림의 상황이 발생하면 어떻게 될까요?

<figure>
    <img src="/posts/study/machine learning/reinforcement learning/images/introduction_to_rl_9.png"
         title="Example of vector reward problem"
         alt="Image of example of vector reward problem"
         class="img_center"
         style="width: 40%"/>
    <figcaption>무단횡단을 하는 사람</figcaption>
</figure>

사실 [Fig. 4.]의 경우는 인공 지능의 윤리성과 연관이 있는 문제이지만, 예시를 위해서 든 경우이기에 인공 지능의 윤리성은 배제하고 설명해보겠습니다. [Fig. 4.]에는 무단횡단을 하는 사람과 자율주행하는 에이전트가 부딪히게 되는 상황이 나타나 있습니다. 당연히 사람과 부딪히면서 교통사고가 나는 것은 절대 학습되어서는 안될 **나쁜** 액션입니다. 하지만 위에서도 언급했듯이 보상에는 **어떻게**가 아닌 **얼마나** 좋은 액션인지를 나타내는 값이 담겨있습니다. 이 예시에서 에이전트가 취할 수 있는 액션은 앞으로 전진하거나 멈춘다는 두가지 액션만 존재한다고 가정해보겠습니다. 그렇다면 2가지 액션에 대한 보상은 아래와 같습니다.

$$
r_t = \begin{bmatrix}
  5 \\
  0
\end{bmatrix}\ (a_t=\text{go straight}), \\
r_t = \begin{bmatrix}
  0 \\
  1
\end{bmatrix}\ (a_t=\text{stop}). \label{vector_reward_2} \tag{2}
$$

식 $(\ref{vector_reward_2})$에서는 2가지 액션에 대한 각각의 보상이 나와있습니다. 에이전트가 취할 수 있는 액션에 대해서 두 보상 모두 **양의** 보상을 받습니다. 그렇다면 어떤 액션이 해당 상태에서 **최적**일까요? 멈추지않고 직진하는 것도 누적 보상이 커지고, 사람을 피해서 멈추는 것도 누적 보상이 커지게 하는 액션입니다. 따라서 해당 상태와 액션에 대해서 **최적의 정책**이나 **가치 함수**가 정해질 수 없습니다. 한 번의 타임 스텝동안에는 단 **하나의 액션**만 취할 수 있기 때문에 보상은 벡터가 아닌 **스칼라**값이 되어야 합니다.

> 최적 정책이나 가치 함수에 대해서는 나중에 자세히 다루겠습니다🙂.

위의 예시같은 경우에는 $r_t=0.5\times 5t+0.5\times h_t$와 같이 가중치를 둔다면 여러 보상의 값을 하나의 스칼라로 표현할 수 있습니다. 만일 어떤 문제는 도저히 하나의 목표로 설정하기 어렵다면 그 문제에 강화 학습을 적용하는 것 자체가 적절하지 않을수도 있습니다.

<i class="fa-solid fa-3"></i>. 희소하고 지연된 보상.

보상의 세 번째 특징은 보상이 **희소**(sparse)할 수 있으며 **지연**(delay)될 수 있다는 점입니다. 액션과 보상이 일대일로 대응이 된다면 강화 학습은 한결 쉬워집니다. 에이전트가 행한 액션과 보상이 즉각적으로 연결되기 때문에 어떤 액션이 좋은 액션인지 가려내기 쉽기 때문입니다. 하지만 보상은 선택했던 액션의 빈도에 비해 훨씬 **가끔 주어지거나**, 액션이 발생한 후 **한참 뒤에** 나올수도 있습니다. 예를 들어 액션을 5번 취한후에 보상을 1번 받거나 액션을 10번하고 나서야 과거의 처음 액션에 대한 보상이 주어진다면, 이 보상이 어떤 액션때문에 받은건지 **구별하기 어렵기** 때문에 학습이 어려워집니다.

강화 학습은 지도 학습과는 다르게 순차적 의사결정 문제를 다루기 때문에, 시간에 따른 흐름에서 보상이 뒤늦게 주어지는 일이 발생할 수 있는 것입니다.

---

이렇게 강화 학습에서 사용되는 **기본 개념**들에 대해서 알아보았습니다. 각각의 요소들이 무엇이고 어떤 특징을 가지고 있는지, 그리고 각각의 요소들이 어떻게 서로 상호작용을 하는지에 대해서 잘 알아두는 것은 강화 학습의 본질적인 원리에 대한 이해를 도와줄 수 있습니다. 이러한 개념들에 대해서 좀 더 구체적이고 이들을 수식화 할 수 있는 **마르코프 결정 프로세스**(Markov Decision Process, MDP)에 대한 내용은 다음 post부터 다뤄보겠습니다.