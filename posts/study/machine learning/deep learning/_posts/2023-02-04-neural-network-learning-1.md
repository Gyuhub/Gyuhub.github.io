---
layout: post
title: Neural-Network-Learning-1
category: deep learning
post-order: 8
---

# 신경망 학습

지금까지는 신경망의 순전파에 대해서 다뤄봤었습니다. 하지만 지금부터는 **신경망 학습**에 대해서 다뤄보겠습니다. 여기서 **학습**이란 <ins>*신경망이 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 갱신*</ins>하는 것을 말합니다. 또한 신경망이 학습의 정도를 파악하는 **지표**인 **손실 함수**(loss function)와 그 손실 함수의 값을 작게 만드는 기법으로 **함수의 기울기**를 활용한 **경사법**에 대해서 알아보겠습니다.

## 데이터 주도 학습

신경망 학습의 특징은 바로 **데이터**를 통해 학습할 수 있다는 점입니다. 데이터에서 학습한다는 것은 **가중치 매개변수**의 값을 사람이 **직접** 정하지 않고 데이터를 보고 **자동**으로 결정한다는 말입니다.

> :bulb: 신경망에 사용되는 매개변수의 개수가 적게는 수천에서 수만, 많게는 수억에도 이를 수 있다는 사실을 생각하면 참 반가운 소식입니다. 😂

**데이터**는 기계학습에서 매우 중요합니다. 데이터에서 **답**을 찾고 데이터에서 **패턴**을 발견하고 데이터로 **이야기**를 만드는, 그것이 바로 기계학습입니다. 데이터가 이끄는 접근 방식덕에 사람 중심 접근에서 벗어나 새로운 관점을 제시할 수 있습니다. 기존의 문제 해결 방식은 **사람의 경험**과 **직관**에 의존했습니다. 반면 기계학습의 문제 해결 방식은 사람의 개입을 **최소화**하고 수집한 데이터로부터 **패턴**을 찾으려 시도합니다. 더 나아가 신경망과 딥러닝은 기존 기계학습의 방식보다 더욱 사람의 개입을 **배제**할 수 있게 해주는 중요한 특성을 지녔습니다.

이를 이미지 인식이라는 주제를 가지고 그림을 통해 설명하면 아래와 같습니다.

<figure>
    <img src="/posts/study/machine%20learning/deep%20learning/images/"
         title="data-centerized learning"
         alt="Image of data-centerized learning"
         class="img_center"
         style="width: 50%"/>
    <figcaption>패러다임의 전환</figcaption>
</figure>

[Fig. 1.]을 보면 첫번째 방식은 **사람**이 모든 알고리즘을 "**설계**"합니다. 이미지에 숨은 **규칙성**을 명확한 **로직**으로 만들어내서 원하는 결과가 나올 수 있게 만드는 것입니다. 하지만 이 방법은 매우 번거롭고 고단한 일임을 금방 알 수 있습니다. 모든 상황과 조건에 대응하면서 문제에 바뀌어도 똑같이 적용될 수 있는 규칙을 만드는 일은 거의 불가능에 가깝다고도 생각할 수 있습니다.

두번째 방식은 이를 개선하여 **사람**이 이미지에서 특정한 **규칙**을 찾아내고, 그 특징의 패턴을 **기계학습**을 통해 해결하는 것입니다. 이미지의 경우 컴퓨터 비전 분야에서는 SIFT, SURF, HOG 등의 변환기를 사용해서 입력 데이터(이미지)에서 본질적인 데이터를 추출합니다. 그리고 이를 가지고 SVM, KNN 등의 지도 학습 방식으로 학습을 진행합니다. 이렇게 기계학습에서는 모아진 데이터로부터 규칙을 찾아내는 역할을 **기계**가 담당합니다. 이 방식은 첫번째 방식에 비해 효율도 높고 사람의 부담도 덜합니다. 하지만 여전히 이미지 변환기를 설계하는 것은 **사람**의 몫임을 주의해야 합니다. 즉, *적절한 변환기*가 설계되지 않는다면 학습이 잘 안될수도 있는 가능성이 있습니다.

세번째 방식은 바로 **사람**의 개입이 없는 신경망(딥러닝)을 사용하여 문제를 해결하는 것입니다. 지금까지의 접근 방식은 **사람**이 문제 해결에 직접 **설계**하는 과정이 포함되었지만, 이 방식은 이미지에 포함된 중요한 특징까지도 **기계**가 스스로 학습할 것입니다. 이 방식의 장점은 모든 문제를 같은 맥락에서 접근할 수 있습니다. 숫자를 인식하거나 얼굴을 인식하거나 세부사항에 관계없이 주어진 데이터를 온전히 학습하여서 패턴을 발견하려고 노력할 것입니다.

> 📝 딥러닝을 **종단간 기계학습**(end-to-end machine learning)이라고도 합니다. 즉, 사람의 개입 없이 모든 문제를 주어진 데이터 그대로를 입력 데이터로 활용해서 목표한 결과를 얻는다는 뜻입니다.

## 훈련 데이터와 시험 데이터

신경망 학습에 대한 설명에 앞서, 기계학습에서 데이터를 취급할 때 주의해야 할 점이 있습니다. 바로 기계학습 문제는 데이터를 **훈련 데이터**(training data)와 **시험 데이터**(test data)로 나누어서 학습과 실험을 수행한다는 것입니다. 그 이유는 바로 기계학습의 목적은 범용적으로 사용할 수 있는 모델을 만들어내는 것입니다. 범용 능력이란 아직 보지 못한 데이터(훈련 데이터에 포함되지 않는 데이터)로도 문제를 올바르게 풀어내는 능력을 말합니다. 따라서 훈련 데이터만 사용하여 학습하면서 최적의 매개변수를 찾고, 시험 데이터를 사용하여 훈련한 모델의 성능을 평가하는 것입니다. 만약 훈련 데이터만 잘 판별하고 시험 데이터는 잘 판별하지 못한다면 모델이 훈련 데이터에만 지나치게 최적화된 **오버피팅**(overfitting)이라는 문제가 발생했다고 생각할 수 있습니다.

> :bell: 오버피팅은 과적합, 과대적합, 과학습, 과적응 등 다양하게 번역됩니다. 오버피팅을 피하는 일은 기계학습의 중요한 과제 중 하나입니다.

## 손실 함수

신경망 학습에서는 현재 학습 상태를 하나의 지표로 표현합니다. 그리고 그 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것입니다. 이 지표를 바로 **손실 함수**(loss function)라고 합니다. 이 손실 함수는 임의의 함수를 사용할 수도 있지만 일반적으로는 **오차제곱합**(SSE)과 **교차 엔트로피 오차**(CEE)를 사용합니다.

> 📑 손실 함수는 신경망 성능의 '나쁨'을 나타내는 지표로, 현재의 신경망이 훈련 데이터를 얼마나 잘 처리하지 '못'하느냐를 나타냅니다. 손실 함수에 마이너스만 곱하면 훈련 데이터를 얼마나 잘 처리하느냐를 나타내는 지표, 즉 신경망 성능의 '좋음'을 나타내는 지표로 활용할 수 있습니다.

### 오차제곱합

가장 많이 쓰이는 손실 함수는 **오차제곱합**(Sum of Squares for Error, SSE)입니다. 수식으로는 아래와 같습니다.

$$
E=\frac{1}{2} \sum_k{(y_k - t_k)^2} \label{SSE} \tag{1}
$$

여기서 $y_k$는 신경망의 출력, $t_k$ 정답 레이블, $k$는 데이터의 차원 수를 나타냅니다. 신경망의 출력과 실제 정답과의 차이(오차)를 계산해서 제곱하여 다 더해주는 과정에서 $\frac{1}{2}$가 갑자기 나오는 것에 의문을 가질 수 있습니다. 이는 [Delta Rule](https://en.wikipedia.org/wiki/Delta_rule)에 의한 것으로, 쉽게 말하자면 나중에 다룰 경사 하강법에서 함수가 **미분**이 되는 과정이 있는데 그 때 발생할 수 있는 기울기의 오차를 최소화하기 위해서 곱하는 것입니다.

#### 구현

Python을 이용해서 오차제곱합 함수를 구현해보겠습니다.

```python
def sum_squares_error(y, t):
    return 0.5 * np.sum((y-t)**2)
```

### 교차 엔트로피 오차

또 다른 손실 함수로서 **교차 엔트로피 오차**(Cross Entropy Error, CEE)도 자주 이용합니다. 수식으로는 아래와 같습니다.

$$
E=-\sum_k{t_k \log{y_k}} \label{CEE} \tag{2}
$$

여기서 $\log{}$는 밑이 자연상수 $e$인 자연로그, $y_k$는 신경망의 출력, $t_k$는 정답 레이블입니다.

#### 구현

Python을 이용해서 교차 엔트로피 오차 함수를 구현해보겠습니다.

```python
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))
```

여기서 $\text{delta}$는 $\log{}$함수에 입력으로 $0$이 들어가는 것을 방지하기 위해서 아주 작은 값으로 사용되었습니다.

### 미니배치 학습

기계학습 문제는 훈련 데이터를 사용해서 학습합니다. 즉, 훈련 데이터가 100개 있다면 그로부터 계산한 100개의 손실 함수의 값들의 함을 지표로 삼는 것입니다. 지금까지는 데이터 하나에 대한 손실 함수만 생각했으나, 이제 훈련 데이터 **모두**에 대한 손실 함수의 합을 구하는 방법에 생각해보겠습니다. 오차제곱합과 교차 엔트로피 오차 함수의 수식은 각각 아래와 같이 바뀝니다.

$$
E=\frac{1}{N} \sum_n{\sum_k{(y_{nk} - t_{nk})^2}} \label{MSE} \tag{3}
$$
이전의 오차제곱합에는 Delta Rule에 의해서 $\frac{1}{2}$를 곱했지만, 미니배치의 평균을 구하는 평균제곱합(MSE)는 $\frac{1}{N}$이 곱해지므로 굳이 $\frac{1}{2}$를 곱하지 않아도 됩니다.
$$
E=-\frac{1}{N} \sum_n{\sum_k{t_{nk} \log{y_{nk}}}} \label{MEE} \tag{4}
$$

여기에서 $N$은 데이터의 개수이고 $t_{nk}$는 $n$번째 데이터의 $k$번째 값을 의미합니다($y_{nk}$는 신경망의 출력, $t_{nk}$는 정답 레이블). 위 수식은 결국 식 $(\ref{SSE})$과 $(\ref{CEE})$를 단순히 $N$개의 데이터로 확장하고 다시 $N$으로 나누어서 정규화하고 있습니다. $N$으로 나눔으로써 **평균 손실 함수**를 구하는 것입니다. 이렇게 평균을 구하면 데이터 개수와 관계없이 항상 통일된 지표를 얻을 수 있습니다.

하지만 일반적으로 데이터셋의 크기는 매우 큽니다. MNIST 데이터셋만 해도 60,000개였습니다. 그래서 모든 데이터를 대상으로 손실 함수의 합을 계산하는 일은 시간이 매우 오래 걸립니다. 이런 경우 데이터의 일부를 추려서 전체의 **근사치**로 이용할 수 있습니다. 신경망 학습에서 훈련 데이터의 일부만 골라 학습을 진행할때 이 일부를 **미니배치**(mini-batch)라고 합니다. 예를 들면 60,000장의 훈련 데이터에서 100장을 **무작위**로 뽑아서 학습을 진행하는데 이를 **미니배치 학습**이라고 합니다.

#### 구현

Python을 이용해서 미니배치 학습에 이용할 교차 엔트로피 오차 함수를 구현해보겠습니다.

```python
def cross_entropy_error(y, t):
    if y.dim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)

    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
```

### 손실 함수의 용도

그런데 손실 함수는 **왜** 사용하는 것일까요? 신경망의 궁극적인 목적은 어떤 입력을 넣었을 때 높은 **정확도**의 출력이 나올 수 있게 끌어내는 매개변수의 값을 찾는 것입니다. 굳이 **정확도**라는 확실하고 명확한 지표가 있는데 **손실 함수의 값**이라는 다소 우회적인 방법을 택하는 걸까요?

그 의문은 신경망 학습에서 **미분**의 역할에 주목한다면 해결됩니다. 신경망 학습에서는 **최적의 매개변수**(가중치와 편향)를 탐색할 때 <ins>손실 함수의 값을 가능한 작게 하는</ins> 매개변수 값을 찾습니다. 이때 <ins>가중치 매개변수의 손실 함수</ins>의 **미분**(정확히는 **기울기**)을 계산하고, 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복합니다. 가중치 매개변수의 손실 함수의 미분이란 "<ins>가중치 매개변수의 값을 아주 조금 변화시켰을 때, 손실 함수가 변하는 정도</ins>"를 말합니다. 만약 이 미분 값이 **음수**면 가중치 매개변수를 **양**의 방향으로, 미분 값이 **양수**라면 가중치 매개변수를 **음**의 방향으로 변화시켜 손실 함수의 값을 줄여나가는 것이죠. 하지만 미분 값이 **0**이라면 가중치 매개변수를 어느 쪽으로 움직여도 손실 함수의 값은 줄어들지 않습니다.

> 💡 만약 정확도를 지표로 삼는다면 미분 값이 대부분의 장소에서 **0**이 되어서 매개변수를 갱신할 수 없습니다. 그래서 신경망을 학습할 때 정확도를 지표로 삼지 않습니다.

좀 더 구체적으로 예를 들어보겠습니다. 한 신경망이 100장의 훈련 데이터 중 32장을 올바로 인식한다고 합니다. 그렇다면 이 신경망의 현재 **정확도**는 **32%**라고 할 수 있겠습니다. 만약 정확도를 지표로 삼아서 학습을 진행한다면 가중치 매개변수의 값을 아주 조금 바꾼다고 해도 정확도는 여전히 **32%**일 것 입니다. 혹, 정확도가 개선된다 하더라도 그 값은 32.1215134%와 같이 **연속적인** 값이 아닌 33% 혹은 31%처럼 **불연속적**이고 **띄엄띄엄한** 값으로 바뀔 것입니다.

> 📝 그래서 정확도를 지표로 삼으면 가중치 매개변수에 대한 손실 함수의 미분 값은 대부분의 장소에서 0이 되는겁니다.

하지만, 손실 함수를 지표로 삼는다면 어떻게 될까요? 현재 손실 함수의 값은 예를 들어 0.6586... 와 같은 수치로 나타날 것입니다. 그리고 매개변수의 값이 조금 변하면 그에 반응해서 손실 함수의 값도 0.6835...처럼 연속적으로 변화하는 것입니다.

> 🤷‍♂️ 계단 함수를 활성화 함수로 사용하지 않는 이유도 이와 같이 신경망 학습이 잘 이뤄지지 않기 때문입니다. 계단 함수의 미분 값은 대부분의 장소에서 0이 되기 때문입니다. 매개변수의 작은 변화가 주는 파장을 계단 함수가 다 없애버리기에 손실 함수의 값에는 아무런 변화가 나타나지 않게 되겠죠.