---
layout: post
title: Neural-Network-Learning-3
category: deep learning
post-order: 10
---

지난 [post](https://gyuhub.github.io/posts/study/machine%20learning/deep%20learning/neural-network-learning-2)에서는 수치 미분과 기울기(gradient), 그리고 경사법에 대해서 알아보았었습니다. 이번 post에서는 신경망 학습 알고리즘을 구현하고 결과를 분석해보겠습니다.

---

# 신경망 학습 알고리즘

구현에 앞서, 신경망 학습의 순서를 정리해보겠습니다.

* 전제
  * 신경망에는 적응 가능한 **매개변수**(가중치와 편향)가 있고, 이 매개변수를 **훈련 데이터**에 적응하도록 조정하는 것을 학습이라고 합니다. 신경망 학습은 다음과 같이 4단계로 수행됩니다.
* 1단계 - 미니배치
  * 훈련 데이터 중 일부(**미니배치**)를 무작위로 가져옵니다. 이 미니배치의 매개변수에 대한 손실 함수 값을 줄이는 것이 목표입니다.
* 2단계 - 기울기 산출
  * 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 **기울기**를 구합니다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시합니다.
* 3단계 - 매개변수 갱신
  * 가중치 매개변수를 기울기 방향으로 **아주 조금** 갱신합니다(경사 하강법).
* 4단계 - 반복
  * 1~3단계를 반복합니다.

1단계에서 미니배치로 무작위로 훈련 데이터를 선정하기 때문에 위 방법을 **확률적 경사 하강법**(Stochastic Gradient Descent, SGD)이라고 부릅니다.

---

이제 손글씨 숫자를 학습하는 신경망을 구현해보겠습니다. MNIST 데이터셋을 가지고 **학습**을 진행하는 신경망의 구조는 아래와 같습니다.

- 2층 네트워크 (입력층, 은닉층 1개, 출력층)
- 입력층의 뉴런 784개 (28x28 pixels)
- 은닉층의 뉴런의 개수 100개
- 출력층의 뉴런 10개 (0에서 9까지의 숫자)
- 활성화 함수는 시그모이드 함수, 출력층 활성화 함수는 소프트맥스 함수
- 가중치 매개변수는 정규분포를 따르는 난수, 편향 매개변수는 0으로 초기화
- 손실 함수는 교차 엔트로피 오차 함수
- 미니배치의 크기는 100, 학습률은 0.1, 경사법에 의한 갱신 횟수는 10,000
- 훈련 데이터의 크기는 60,000, 시험 데이터의 크기는 10,000
  
자세한 코드는 Git 저장소[^fn-two-layered-network-learning]를 통해 확인해주시기 바랍니다.

아래의 그림은 손실 함수의 값의 변화를 기록한 그래프[^fn-numerical-gradient]입니다.

<figure>
    <img src="/posts/study/machine%20learning/deep%20learning/images/neural-network-learning_7.jpg"
         title="Graph of loss function"
         alt="Image of graph of loss function"
         class="img_center"
         style="width: 50%"/>
    <figcaption>학습에 의한 손실 함수값의 변화</figcaption>
</figure>

[Fig. 1.]을 보면 학습 횟수가 늘어나면서 손실 함수값이 줄어듭니다. 이는 학습이 잘 되고 있다는 뜻으로, 신경망의 가중치 매개변수가 서서히 데이터에 적응하고 있음을 의미합니다. 신경망이 학습을 하고 있다는 의미입니다.

하지만 위에서 나타낸 손실 함수의 값은, 정확히는 "**훈련 데이터의 미니배치**"에 대한 손실 함수의 값입니다. 훈련 데이터에 대해서는 잘 학습하고 있어도 다른 데이터셋에도 비슷한 성능을 발휘할지는 의문입니다.

신경망 학습에서는 훈련 데이터 외의 데이터에도 올바르게 적응하는지를 확인해야 합니다. 즉, 훈련 데이터에 "**오버피팅**"을 일으키지 않는지 확인해야 합니다. 오버피팅되었다면 훈련 데이터에 포함된 이미지만 잘 구별하고, 그렇지 않은 이미지는 잘 식별할 수 없다는 의미입니다. 신경망 학습의 원래 목표는 **범용적인 능력**을 익히는 것입니다. 따라서 시험 데이터를 가지고 정확도를 판별해보겠습니다.

> 💡 훈련 데이터와 시험 데이터를 1에폭별로 기록할 예정입니다. 여기서 **에폭**(epoch)이란 하나의 단위입니다. 1에폭은 학습에서 **훈련 데이터**를 **모두** 소진했을 때의 횟수에 해당합니다. 예를 들어 훈련 데이터 10,000개를 100개의 미니배치로 학습할 경우, 확률적 경사 하강법을 100회 반복하면 모든 훈련 데이터를 '**소진**'한게 됩니다. 이 경우 100회가 1에폭이 됩니다. 수식으로 표현하면 <ins>(전체 데이터셋 크기)/(미니배치의 크기)=(1에폭의 크기)</ins>라고 말할 수 있겠습니다.

아래의 그림은 훈련 데이터와 시험 데이터에 대한 정확도를 기록한 그래프[^fn-numerical-gradient]입니다.

<figure>
    <img src="/posts/study/machine%20learning/deep%20learning/images/neural-network-learning_8.jpg"
         title="Graph of accuracy"
         alt="Image of graph of accuracy"
         class="img_center"
         style="width: 50%"/>
    <figcaption>훈련 데이터 vs 시험 데이터</figcaption>
</figure>

[Fig. 2.]에서 실선은 훈련 데이터, 점선은 시험 데이터에 대한 정확도를 나타냈습니다. 학습이 진행될수록 두 데이터셋에 대한 정확도가 모두 좋아지는 것을 확인할 수 있습니다. 즉, 오버피팅이 일어나지 않았다고 생각할 수 있겠습니다.

> 💡 만약 오버피팅이 일어난다면 어느 시점부터 **시험 데이터**에 대한 정확도는 떨어질 것입니다. 이 순간이 오버피팅이 **시작**되는 순간이라고 생각할 수 있겠습니다. 이러한 순간을 포착해 학습을 **중단**하면 오버피팅을 예방할 수 있는데 이를 **조기 종료**(early stopping)라고 하며, 나중에 다룰 **가중치 감소**, **드롭아웃**과 함께 대표적인 오버피팅 예방법입니다.

지금까지 신경망 학습의 기초와 이론, 구현까지 정리해봤습니다. 다음 post에서는 효과적인 기울기 계산을 위한 **오차역전파법**(backpropagation)에 대해서 본격적으로 다뤄보겠습니다.

## 신경망 학습 요약
- 기계학습에서 사용하는 데이터셋은 훈련 데이터와 시험 데이터로 나누어서 사용한다.
- 훈련 데이터로 학습한 모델의 범용 능력을 시험 데이터로 평가한다.
- 신경망 학습은 손실 함수를 지표로, 손실 함수의 값이 작아지는 방향으로 가중치 매개변수를 갱신한다.
- 가중치 매개변수를 갱신할 때는 가중치 매개변수의 기울기를 이용하고, 기울어진 방향으로 가중치의 값을 갱신하는 작업을 반복한다.
- 수치 미분을 이용해서 가중치 매개변수의 기울기를 구하는 방법도 있다.
- 수치 미분을 통한 계산은 구현이 간단하지만 시간이 오래 걸린다. 오차역전파법은 구현이 어렵지만 기울기를 고속으로 계산할 수 있다.

---

[^fn-two-layered-network-learning]: [Github repository](https://github.com/Gyuhub/dl_scratch/blob/main/dl_scratch/ch4/two_layered_network_learning.py)에서 2층 신경망 학습에 관한 코드를 확인하실 수 있습니다.

[^fn-numerical-gradient]: 😢 사실 원래라면 수치 미분을 이용한 기울기로 계산하려고 했지만 실제 코드로 구현해보니 연산시간이 말도 안되게 오래 걸려서, [저자의 저장소](https://github.com/WegraLee/deep-learning-from-scratch/blob/master/ch04/two_layer_net.py)의 오차역전파법을 이용한 기울기를 사용해서 구했습니다.